# 向量数据库配置与使用指南

## 一、向量数据库简介

向量数据库（Vector Database）用于存储和检索文本的向量表示（Embeddings），适用于：
- 文档存储与语义搜索
- 知识库管理
- 相似度匹配
- RAG（检索增强生成）应用

本项目使用 **ChromaDB** 作为向量数据库。

## 二、安装依赖

### 1. 安装Python包

```bash
cd backend
pip install python-docx chromadb sentence-transformers
```

**包说明：**
- `python-docx`: 解析Word文档
- `chromadb`: 向量数据库
- `sentence-transformers`: 文本向量化（将文本转换为向量）

### 2. 下载向量化模型（首次使用）

```bash
python -c "from sentence_transformers import SentenceTransformer; model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2'); print('模型下载完成')"
```

这会下载一个支持中文的多语言模型（约420MB），首次运行需要等待。

## 三、ChromaDB基本使用

### 1. 创建向量数据库客户端

```python
import chromadb
from chromadb.config import Settings

# 持久化存储
client = chromadb.PersistentClient(
    path="./data/chroma_db",  # 数据存储路径
    settings=Settings(
        anonymized_telemetry=False
    )
)

# 创建或获取集合（Collection）
collection = client.get_or_create_collection(
    name="survey_documents",
    metadata={"description": "问卷文档知识库"}
)
```

### 2. 添加文档

```python
from sentence_transformers import SentenceTransformer

# 加载向量化模型
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# 文档内容
documents = ["这是第一个问题", "这是第二个问题"]
ids = ["q1", "q2"]
metadatas = [
    {"type": "question", "difficulty": "easy"},
    {"type": "question", "difficulty": "hard"}
]

# 生成向量
embeddings = model.encode(documents).tolist()

# 添加到向量数据库
collection.add(
    documents=documents,
    embeddings=embeddings,
    ids=ids,
    metadatas=metadatas
)
```

### 3. 查询相似文档

```python
# 查询问题
query = "什么是数据结构？"
query_embedding = model.encode([query]).tolist()

# 搜索最相似的5个文档
results = collection.query(
    query_embeddings=query_embedding,
    n_results=5,
    include=["documents", "metadatas", "distances"]
)

print("相似文档：", results['documents'])
print("元数据：", results['metadatas'])
print("相似度分数：", results['distances'])
```

### 4. 更新文档

```python
collection.update(
    ids=["q1"],
    documents=["更新后的问题内容"],
    metadatas=[{"type": "question", "difficulty": "medium"}]
)
```

### 5. 删除文档

```python
collection.delete(ids=["q1"])
```

## 四、项目集成步骤

### 1. 创建向量数据库服务

创建文件：`backend/app/services/vector_db_service.py`

```python
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Any
import os

class VectorDBService:
    def __init__(self):
        # 数据存储路径
        db_path = os.path.join(os.getcwd(), "data", "chroma_db")
        os.makedirs(db_path, exist_ok=True)
        
        # 创建ChromaDB客户端
        self.client = chromadb.PersistentClient(
            path=db_path,
            settings=Settings(anonymized_telemetry=False)
        )
        
        # 加载向量化模型
        self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        
        # 创建问卷文档集合
        self.collection = self.client.get_or_create_collection(
            name="survey_documents",
            metadata={"description": "问卷文档知识库"}
        )
    
    def add_document(self, doc_id: str, content: str, metadata: Dict[str, Any]) -> bool:
        """添加文档到向量数据库"""
        try:
            # 生成向量
            embedding = self.model.encode([content]).tolist()[0]
            
            # 添加到数据库
            self.collection.add(
                ids=[doc_id],
                documents=[content],
                embeddings=[embedding],
                metadatas=[metadata]
            )
            return True
        except Exception as e:
            print(f"添加文档失败: {e}")
            return False
    
    def search_similar(self, query: str, n_results: int = 5) -> List[Dict[str, Any]]:
        """搜索相似文档"""
        try:
            # 生成查询向量
            query_embedding = self.model.encode([query]).tolist()
            
            # 查询
            results = self.collection.query(
                query_embeddings=query_embedding,
                n_results=n_results,
                include=["documents", "metadatas", "distances"]
            )
            
            # 格式化结果
            formatted_results = []
            for i in range(len(results['ids'][0])):
                formatted_results.append({
                    "id": results['ids'][0][i],
                    "content": results['documents'][0][i],
                    "metadata": results['metadatas'][0][i],
                    "similarity": 1 - results['distances'][0][i]  # 转换为相似度分数
                })
            
            return formatted_results
        except Exception as e:
            print(f"搜索失败: {e}")
            return []

# 创建全局实例
vector_db = VectorDBService()
```

### 2. Word文档解析服务

创建文件：`backend/app/services/document_parser.py`

```python
from docx import Document
from typing import List, Dict, Any
import re

class DocumentParser:
    @staticmethod
    def parse_word(file_path: str) -> List[Dict[str, Any]]:
        """解析Word文档，提取问题"""
        try:
            doc = Document(file_path)
            questions = []
            
            current_question = None
            current_options = []
            
            for para in doc.paragraphs:
                text = para.text.strip()
                if not text:
                    continue
                
                # 检测问题（以数字开头）
                question_match = re.match(r'^(\d+)[.、](.+)', text)
                if question_match:
                    # 保存上一个问题
                    if current_question:
                        questions.append({
                            "question": current_question,
                            "options": current_options,
                            "type": "single_choice" if len(current_options) > 0 else "text"
                        })
                    
                    current_question = question_match.group(2).strip()
                    current_options = []
                
                # 检测选项（A. B. C. D.）
                option_match = re.match(r'^([A-Z])[.、](.+)', text)
                if option_match and current_question:
                    current_options.append({
                        "label": option_match.group(1),
                        "text": option_match.group(2).strip()
                    })
            
            # 添加最后一个问题
            if current_question:
                questions.append({
                    "question": current_question,
                    "options": current_options,
                    "type": "single_choice" if len(current_options) > 0 else "text"
                })
            
            return questions
            
        except Exception as e:
            raise Exception(f"Word文档解析失败: {str(e)}")

# 创建全局实例
doc_parser = DocumentParser()
```

### 3. 创建API端点

在 `backend/app/api/teacher/survey.py` 中添加：

```python
from fastapi import UploadFile, File
from app.services.document_parser import doc_parser
from app.services.vector_db_service import vector_db
import os
import uuid

@router.post("/upload-word")
async def upload_word_document(file: UploadFile = File(...)):
    """上传Word文档并解析"""
    try:
        # 保存临时文件
        temp_dir = "temp_uploads"
        os.makedirs(temp_dir, exist_ok=True)
        
        file_path = os.path.join(temp_dir, f"{uuid.uuid4()}.docx")
        
        with open(file_path, "wb") as f:
            content = await file.read()
            f.write(content)
        
        # 解析Word文档
        questions = doc_parser.parse_word(file_path)
        
        # 存储到向量数据库
        doc_id = str(uuid.uuid4())
        full_content = "\n".join([q["question"] for q in questions])
        
        vector_db.add_document(
            doc_id=doc_id,
            content=full_content,
            metadata={
                "filename": file.filename,
                "question_count": len(questions),
                "upload_time": datetime.now().isoformat()
            }
        )
        
        # 删除临时文件
        os.remove(file_path)
        
        return {
            "success": True,
            "doc_id": doc_id,
            "questions": questions,
            "message": f"成功解析 {len(questions)} 个问题"
        }
        
    except Exception as e:
        return {"success": False, "message": str(e)}
```

## 五、前端集成

在 `frontend/src/pages/teacher/Survey/index.tsx` 中添加文件上传和问题编辑功能。

## 六、使用流程

1. 教师上传Word文档
2. 后端解析Word，提取问题
3. 将文档存储到向量数据库
4. 前端展示解析结果，允许编辑
5. 确认后保存到PostgreSQL数据库

## 七、向量数据库维护

### 查看所有文档

```python
# 获取集合中的所有文档
results = collection.get(include=["documents", "metadatas"])
print(f"总文档数: {len(results['ids'])}")
```

### 清空集合

```python
client.delete_collection("survey_documents")
```

### 备份数据

ChromaDB的数据保存在 `./data/chroma_db` 目录，可以直接复制该目录进行备份。

## 八、注意事项

1. **首次使用需要下载模型**，大约420MB，需要网络连接
2. **向量数据库占用磁盘空间**，定期清理不需要的文档
3. **搜索性能**：ChromaDB适合中小规模数据（<100万条）
4. **中文支持**：使用 `paraphrase-multilingual-MiniLM-L12-v2` 模型支持中文

## 九、常见问题

### Q: 模型下载太慢？
A: 可以使用国内镜像或手动下载模型文件。

### Q: 如何提高搜索准确度？
A: 使用更大的向量模型，或使用OpenAI的embedding API。

### Q: 向量数据库能存储多少文档？
A: ChromaDB理论上无限制，但建议<100万条文档以保证性能。
