#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é‡æ–°å¤„ç†å·²ä¸Šä¼ çš„PPTæ–‡æ¡£,æå–çœŸå®å†…å®¹
"""

import psycopg2
import os
import sys

# æ·»åŠ appç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'app'))

from services.document_processor import DocumentProcessor
from services.vector_db_service import VectorDBService

DATABASE_URL = "postgresql://postgres:123456@localhost:5432/app_project"

def reprocess_documents():
    """é‡æ–°å¤„ç†æ‰€æœ‰PPTæ–‡æ¡£"""
    try:
        conn = psycopg2.connect(DATABASE_URL)
        conn.autocommit = False
        cursor = conn.cursor()
        
        print("=" * 60)
        print("æŸ¥æ‰¾éœ€è¦é‡æ–°å¤„ç†çš„æ–‡æ¡£...")
        print("=" * 60)
        
        # æŸ¥è¯¢æ‰€æœ‰éœ€è¦é‡æ–°å¤„ç†çš„PPTæ–‡æ¡£
        cursor.execute("""
            SELECT id, course_id, file_name, file_path, file_type
            FROM course_documents
            WHERE file_type = '.pptx'
            AND processed_status = 'completed'
            ORDER BY created_at;
        """)
        
        documents = cursor.fetchall()
        print(f"\næ‰¾åˆ° {len(documents)} ä¸ªPPTæ–‡æ¡£éœ€è¦é‡æ–°å¤„ç†\n")
        
        if not documents:
            print("æ²¡æœ‰éœ€è¦å¤„ç†çš„æ–‡æ¡£")
            return
        
        # åˆå§‹åŒ–æœåŠ¡
        processor = DocumentProcessor()
        vector_service = VectorDBService()
        
        success_count = 0
        fail_count = 0
        
        for doc_id, course_id, file_name, file_path, file_type in documents:
            print(f"ğŸ”„ å¤„ç†: {file_name}")
            print(f"   è·¯å¾„: {file_path}")
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(file_path):
                print(f"   âŒ æ–‡ä»¶ä¸å­˜åœ¨,è·³è¿‡\n")
                fail_count += 1
                continue
            
            try:
                # 1. åˆ é™¤æ—§çš„çŸ¥è¯†åº“æ•°æ®
                cursor.execute("""
                    DELETE FROM knowledge_base
                    WHERE document_id = %s;
                """, (doc_id,))
                print(f"   ğŸ—‘ï¸  å·²åˆ é™¤æ—§æ•°æ®")
                
                # 2. é‡æ–°æå–æ–‡æ¡£å†…å®¹
                text = processor.extract_text(file_path, file_type)
                print(f"   ğŸ“„ æå–æˆåŠŸ: {len(text)} å­—ç¬¦")
                
                if text:
                    # æ˜¾ç¤ºå‰100å­—ç¬¦
                    preview = text[:100] if text else ""
                    print(f"   å†…å®¹é¢„è§ˆ: {preview}...")
                
                # åˆ‡åˆ†æˆæ–‡æœ¬å—
                chunks_data = processor.split_text_into_chunks(text, {
                    "document_id": str(doc_id),
                    "course_id": str(course_id),
                    "file_name": file_name,
                    "file_type": file_type
                })
                print(f"   âœ‚ï¸  æ–‡æœ¬åˆ‡åˆ†: {len(chunks_data)} ä¸ªæ–‡æœ¬å—")
                
                # 3. ç”Ÿæˆå‘é‡å¹¶ä¿å­˜
                for chunk_data in chunks_data:
                    chunk_text = chunk_data['text']
                    chunk_index = chunk_data['chunk_index']
                    
                    # ç”Ÿæˆå‘é‡
                    embedding = vector_service.model.encode([chunk_text])[0]
                    
                    # ä¿å­˜åˆ°çŸ¥è¯†åº“
                    cursor.execute("""
                        INSERT INTO knowledge_base
                        (document_id, course_id, chunk_text, chunk_index, chunk_metadata, embedding_vector)
                        VALUES (%s, %s, %s, %s, %s, %s)
                    """, (
                        doc_id,
                        course_id,
                        chunk_text,
                        chunk_index,
                        f'{{"document_id": "{doc_id}", "course_id": "{course_id}", "file_name": "{file_name}", "file_type": "{file_type}"}}',
                        str(embedding.tolist())
                    ))
                
                # 4. æ›´æ–°å¤„ç†çŠ¶æ€
                cursor.execute("""
                    UPDATE course_documents
                    SET processed_status = 'completed',
                        updated_at = CURRENT_TIMESTAMP
                    WHERE id = %s
                """, (doc_id,))
                
                conn.commit()
                print(f"   âœ… å¤„ç†å®Œæˆ: {len(chunks_data)} ä¸ªæ–‡æœ¬å—å·²ä¿å­˜\n")
                success_count += 1
                
            except Exception as e:
                conn.rollback()
                print(f"   âŒ å¤„ç†å¤±è´¥: {e}\n")
                fail_count += 1
                import traceback
                traceback.print_exc()
        
        cursor.close()
        conn.close()
        
        print("=" * 60)
        print(f"âœ… é‡æ–°å¤„ç†å®Œæˆ!")
        print(f"   æˆåŠŸ: {success_count} ä¸ª")
        print(f"   å¤±è´¥: {fail_count} ä¸ª")
        print("=" * 60)
        
        # éªŒè¯ç»“æœ
        verify_reprocessing()
        
    except Exception as e:
        print(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def verify_reprocessing():
    """éªŒè¯é‡æ–°å¤„ç†çš„ç»“æœ"""
    try:
        conn = psycopg2.connect(DATABASE_URL)
        cursor = conn.cursor()
        
        print("\n" + "=" * 60)
        print("éªŒè¯å¤„ç†ç»“æœ...")
        print("=" * 60)
        
        cursor.execute("""
            SELECT 
                cd.file_name,
                COUNT(kb.id) as chunk_count,
                LEFT(kb.chunk_text, 80) as sample_text
            FROM course_documents cd
            LEFT JOIN knowledge_base kb ON cd.id = kb.document_id
            WHERE cd.file_type = '.pptx'
            GROUP BY cd.id, cd.file_name, kb.chunk_text
            ORDER BY cd.file_name
            LIMIT 5;
        """)
        
        docs = cursor.fetchall()
        print(f"\nå‰5ä¸ªæ–‡æ¡£çš„å¤„ç†ç»“æœ:\n")
        for doc in docs:
            file_name, chunks, sample = doc
            is_placeholder = "[PowerPointæ–‡ä»¶å†…å®¹æå–éœ€è¦å®‰è£…python-pptxåº“]" in (sample or "")
            status = "âŒ å ä½æ–‡æœ¬" if is_placeholder else "âœ… çœŸå®å†…å®¹"
            print(f"{status} {file_name}")
            print(f"         æ–‡æœ¬å—æ•°: {chunks}")
            print(f"         å†…å®¹: {sample}...\n")
        
        cursor.close()
        conn.close()
        
    except Exception as e:
        print(f"âŒ éªŒè¯å¤±è´¥: {e}")

if __name__ == "__main__":
    print("\nğŸš€ å¼€å§‹é‡æ–°å¤„ç†æ–‡æ¡£...\n")
    reprocess_documents()
